{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLWo3YPkZXbq"
      },
      "source": [
        "\n",
        "This colab lets you upload a paper to your drive and talk to it using Open AI's embeddings. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW8mjR29W-c4"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiiqxuQRgPbz",
        "outputId": "fac2a62a-d241-4f7c-e34e-224e03d1bba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.8/dist-packages (3.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from pypdf) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (3.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.8/dist-packages (3.0.1)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from PyPDF2) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.8/dist-packages (0.1.2)\n",
            "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2.0.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2022.6.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2.28.2)\n",
            "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
            "Requirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (3.9.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (1.26.14)\n",
            "Requirement already satisfied: pycryptodomex~=3.8 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (3.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.8/dist-packages (0.26.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.28.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n",
        "!pip install wget\n",
        "!pip install PyPDF2\n",
        "!pip install tiktoken\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5QmQwevXMBv"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "-pMQIv3jf4eU"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from collections import defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches\n",
        "import argparse\n",
        "from pypdf import PdfReader\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openai \n",
        "import tiktoken\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YixvdHYjXOQU"
      },
      "source": [
        "## Setup\n",
        "Specify api key, mount Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlbOJyU0yIfS",
        "outputId": "06370cbb-75cc-4c2d-ebea-5a8ee718d1e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "openai.api_key = '___'\n",
        "sys.path.append(\"../\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc7svRMQYUw3"
      },
      "source": [
        "## Upload paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "1ptREjxpLp00"
      },
      "outputs": [],
      "source": [
        "filename = '/content/drive/MyDrive/' + '1706.03762.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhCZZLrAYYP-"
      },
      "source": [
        "## Parse PDF to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "DhL684zgLDF-"
      },
      "outputs": [],
      "source": [
        "def parse_paper(path):\n",
        "  print(\"Parsing paper\")\n",
        "  reader = PdfReader(path)\n",
        "  number_of_pages = len(reader.pages)\n",
        "  print(f\"Total number of pages: {number_of_pages}\")\n",
        "  paper_text = []\n",
        "  for i in range(number_of_pages):\n",
        "    page = reader.pages[i]\n",
        "    page_text = []\n",
        "\n",
        "    def visitor_body(text, cm, tm, fontDict, fontSize):\n",
        "      x = tm[4]\n",
        "      y = tm[5]\n",
        "      # ignore header/footer\n",
        "      if (y > 50 and y < 720) and (len(text.strip()) > 1):\n",
        "        page_text.append({\n",
        "          'fontsize': fontSize,\n",
        "          'text': text.strip().replace('\\x03', ''),\n",
        "          'x': x,\n",
        "          'y': y\n",
        "        })\n",
        "\n",
        "    _ = page.extract_text(visitor_text=visitor_body)\n",
        "\n",
        "    blob_font_size = None\n",
        "    blob_text = ''\n",
        "    processed_text = []\n",
        "\n",
        "    for t in page_text:\n",
        "      if t['fontsize'] == blob_font_size:\n",
        "        blob_text += f\" {t['text']}\"\n",
        "      else:\n",
        "        if blob_font_size is not None and len(blob_text) > 1:\n",
        "          processed_text.append({\n",
        "            'fontsize': blob_font_size,\n",
        "            'text': blob_text,\n",
        "            'page': i\n",
        "          })\n",
        "        blob_font_size = t['fontsize']\n",
        "        blob_text = t['text']\n",
        "    paper_text += processed_text\n",
        "  return paper_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qm9GI3TLfeD",
        "outputId": "3e779d23-de3e-418c-f3c4-12c57a9a5cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing paper\n",
            "Total number of pages: 15\n"
          ]
        }
      ],
      "source": [
        "paper_text = parse_paper(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4DhysCqYcup"
      },
      "source": [
        "## Apply a small filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "orUj9AgyLvMy"
      },
      "outputs": [],
      "source": [
        "filtered_paper_text = []\n",
        "for row in paper_text:\n",
        "  if len(row['text']) < 30:\n",
        "    continue\n",
        "  filtered_paper_text.append(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGc-CIqdYgF1"
      },
      "source": [
        "## Convert to dataframe and inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ezUH8kygHDL"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(filtered_paper_text)\n",
        "print(df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zriCg1dxYkE0"
      },
      "source": [
        "## Calculate pdf embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "JSgBs0QVApeE"
      },
      "outputs": [],
      "source": [
        "embedding_model = \"text-embedding-ada-002\"\n",
        "embeddings = df.text.apply([lambda x: get_embedding(x, engine=embedding_model)])\n",
        "df[\"embeddings\"] = embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GBQjg5NGFf4",
        "outputId": "752accff-fdba-49f6-d95a-37dd4b41ac15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(43, 4)"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ga0r7-uY10Z"
      },
      "source": [
        "## Embed query and Search\n",
        "\n",
        "We return the chunk in pdf with highest cosine similarity with query embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "QZZs5pT9-xtl"
      },
      "outputs": [],
      "source": [
        "def search_reviews(df, query, n=3, pprint=True):\n",
        "    query_embedding = get_embedding(\n",
        "        query,\n",
        "        engine=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    df[\"similarity\"] = df.embeddings.apply(lambda x: cosine_similarity(x, query_embedding))\n",
        "\n",
        "    results = (\n",
        "        df.sort_values(\"similarity\", ascending=False)\n",
        "        \n",
        "    )\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTeveJeQZImC"
      },
      "source": [
        "## Few Example Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4-HMlmuZQX7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "9VdQYHZ8_vqd",
        "outputId": "e9abed8b-329e-4541-fe07-b0d1f929c361"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead( Q;K;V ) = Concat(head ;:::; head where head = Attention( QW ;KW ;VW Where the projections are parameter matrices'"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = search_reviews(df, \"explain how multi head self attention works\", n=3)\n",
        "results.iloc[0]['text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "v63tyA1sWwxL",
        "outputId": "9a897eb8-f323-48dd-bf15-1243d899dd02"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the signiï¬cantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [ 20] with = 0 = 0 98 and = 10 . We varied the learning rate over the course of training, according to the formula: lrate'"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = search_reviews(df, \"explain the training procedure\", n=3)\n",
        "results.iloc[0]['text']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
