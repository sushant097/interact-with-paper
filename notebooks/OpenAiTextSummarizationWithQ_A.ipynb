{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-XU3_aRitCbl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675183171790,
     "user_tz": -345,
     "elapsed": 4245,
     "user": {
      "displayName": "Sushant Gautam",
      "userId": "06574835761842355906"
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install --quiet openai"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade --quiet pymupdf"
   ],
   "metadata": {
    "id": "ZSfTikpK0Bu5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675183194188,
     "user_tz": -345,
     "elapsed": 4819,
     "user": {
      "displayName": "Sushant Gautam",
      "userId": "06574835761842355906"
     }
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "OPENAI_API_KEY = ''\n",
    "with open('OpenAi.json', 'r') as file_to_read:\n",
    "    json_data = json.load(file_to_read)\n",
    "    OPENAI_API_KEY = json_data[\"OPENAI_API_KEY\"]"
   ],
   "metadata": {
    "id": "RfmpkMqT1daE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675183138505,
     "user_tz": -345,
     "elapsed": 8,
     "user": {
      "displayName": "Sushant Gautam",
      "userId": "06574835761842355906"
     }
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key =  OPENAI_API_KEY"
   ],
   "metadata": {
    "id": "XajkO-wEtnXp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675183138883,
     "user_tz": -345,
     "elapsed": 385,
     "user": {
      "displayName": "Sushant Gautam",
      "userId": "06574835761842355906"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import fitz"
   ],
   "metadata": {
    "id": "srz45bfF0DJ1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675183138884,
     "user_tz": -345,
     "elapsed": 7,
     "user": {
      "displayName": "Sushant Gautam",
      "userId": "06574835761842355906"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "doc = fitz.open('/content/AttentionAllYouNeed-2017.pdf') "
   ],
   "metadata": {
    "id": "vfWpy7_QVC8t",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675183138884,
     "user_tz": -345,
     "elapsed": 6,
     "user": {
      "displayName": "Sushant Gautam",
      "userId": "06574835761842355906"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"TotalPages of doc: \",len(doc))"
   ],
   "metadata": {
    "id": "I5yndmuLjjob",
    "outputId": "ed382d2a-8a14-4f28-ea15-3750b6cce5b3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675183138885,
     "user_tz": -345,
     "elapsed": 6,
     "user": {
      "displayName": "Sushant Gautam",
      "userId": "06574835761842355906"
     }
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TotalPages of doc:  15\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "summary_list =[]\n",
    "for page in doc:\n",
    "  text = page.get_text(\"text\")\n",
    "  #print(text)\n",
    "  prompt= text + \"\\n Tl;dr:\"\n",
    "  response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prompt,\n",
    "    temperature=0.7,\n",
    "    max_tokens=120,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=1\n",
    "  )\n",
    "  summary_list.append(response[\"choices\"][0][\"text\"])\n"
   ],
   "metadata": {
    "id": "Ni19lQsoVJKP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i, summary_text in enumerate(summary_list):\n",
    "  print(f\"Page {i+1}: \\n {summary_text}\")"
   ],
   "metadata": {
    "id": "umXqR5HYkm_u",
    "outputId": "3730ce33-280c-4184-997b-5c44d7bdc6ef",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Page 1: \n",
      " \n",
      " This paper proposes a new sequence transduction model based on attention mechanisms, which is superior in quality to existing models while requiring significantly less time to train. Experiments on two machine translation tasks show that the Transformer model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task and 41.8 BLEU on the WMT 2014 English-to-French translation task. The model also generalizes well to other tasks, such as English constituency parsing with both large and limited training data.\n",
      "Page 2: \n",
      "  \n",
      " The Transformer is a model architecture that uses self-attention mechanisms to draw global dependencies between input and output, allowing for more parallelization and reaching a new state of the art in translation quality. Previous work such as Extended Neural GPU, ByteNet, and ConvS2S have used convolutional neural networks to compute hidden representations in parallel, but the Transformer takes this one step further by relying entirely on self-attention.\n",
      "Page 3: \n",
      " \n",
      "The Transformer model architecture consists of an encoder and a decoder stack. The encoder is composed of a stack of N=6 identical layers, each of which has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder also has three sub-layers: a multi-head attention layer over the output of the encoder stack, a self-attention layer to prevent positions from attending to subsequent positions, and a fully connected feed-forward network. Attention is a function that maps a\n",
      "Page 4: \n",
      "  Scaled dot-product attention is a type of attention mechanism used in natural language processing and machine learning. It consists of queries, keys, and values which are combined through matrix multiplication and scaled by the square root of the number of dimensions. Multi-head attention uses multiple attention layers running in parallel to increase efficiency.\n",
      "Page 5: \n",
      " \n",
      " Multi-head attention allows the model to attend to different representation subspaces at different positions. It uses multiple attention heads, each of which has its own parameter matrices for the queries, keys and values. These are then concatenated together and applied to the output with a weight matrix. This is used in the encoder-decoder attention layers, self-attention layers in the encoder, and self-attention layers in the decoder with masking to preserve the auto-regressive property. In addition, position-wise feed-forward networks, embeddings,\n",
      "Page 6: \n",
      " \n",
      "This paper compares the use of self-attention layers to recurrent and convolutional layers for sequence transduction tasks. Self-attention layers have a constant computational complexity per layer, whereas recurrent and convolutional layers have O(n) and O(k*n) complexities respectively. In terms of path length between long-range dependencies, self-attention layers have shorter paths than recurrent or convolutional layers. We also discuss the use of positional encodings to improve performance for longer sequences.\n",
      "Page 7: \n",
      "  We describe our training regime for the Transformer model, including data and batching, hardware and schedule, optimizer, and regularization. We use Adam optimizer with warmup steps and residual dropout.\n",
      "Page 8: \n",
      "  The Transformer model achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. It surpasses all previously published models and ensembles with its base model, achieving 28.4 BLEU for the English-to-German translation task and 41.0 BLEU for the English-to-French translation task. Label smoothing during training is also used to improve accuracy and BLEU score.\n",
      "Page 9: \n",
      "  In Table 3, reducing the attention key size dk hurts model quality, suggesting that determining compatibility is not easy. Bigger models are better, and dropout is very helpful in avoiding over-fitting. Replacing our sinusoidal positional encoding with learned positional embeddings results in nearly identical results to the base model. In Table 4, experiments on English constituency parsing show that the Transformer can generalize to other tasks, achieving competitive results.\n",
      "Page 10: \n",
      " \n",
      " \n",
      "This paper presents the Transformer, a sequence transduction model based entirely on attention. It is shown to be able to outperform other architectures based on recurrent or convolutional layers in translation tasks. The Transformer is trained faster and achieves state-of-the-art results for English-to-German and English-to-French translation tasks. We plan to extend the Transformer to other tasks and investigate local, restricted attention mechanisms for large inputs and outputs.\n",
      "Page 11: \n",
      " \n",
      "This paper reviews the major developments in recurrent neural network (RNN) models for sequence modeling. It covers topics such as long short-term memory (LSTM), gated recurrent units (GRUs), and self-attention networks. It also discusses recent applications of RNNs, such as machine translation, natural language understanding, and image captioning. Finally, it provides an overview of some of the most popular datasets used to evaluate RNN models.\n",
      "Page 12: \n",
      " \n",
      " \n",
      "This paper presents a review of recent advances in neural machine translation, summarization, parsing and language modeling. It covers models such as encoder-decoders, attention mechanisms, tree-based models, deep reinforced models, output embeddings, subword units, sparsely-gated mixture-of-experts layers, and end-to-end memory networks. It also discusses popular approaches such as dropout, Google's Neural Machine Translation system, Fast and Accurate Shift-Reduce Constituent Parsing, and Grammar as a Foreign Language.\n",
      "Page 13: \n",
      "  The attention mechanism in layer 5 of the encoder self-attention follows long-distance dependencies, and many of the attention heads attend to a distant dependency of the verb \"making,\" completing the phrase \"making...more difficult.\"\n",
      "Page 14: \n",
      " \n",
      "Attention heads in layer 5 of a 6-layer network appear to be involved in anaphora resolution, as evidenced by the sharp attentions seen when focusing on the word \"its\".\n",
      "Page 15: \n",
      "  Attention heads in the encoder self-attention layers of a Transformer model can be observed to perform different tasks related to the structure of the sentence, demonstrating their ability to learn useful features from the data.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "summary_text=' '.join(summary_list)\n",
    "print(summary_text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUjynEGNXcbT",
    "outputId": "02276b0b-643e-4e88-826b-5cddbc285595"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " This paper proposes a new sequence transduction model based on attention mechanisms, which is superior in quality to existing models while requiring significantly less time to train. Experiments on two machine translation tasks show that the Transformer model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task and 41.8 BLEU on the WMT 2014 English-to-French translation task. The model also generalizes well to other tasks, such as English constituency parsing with both large and limited training data.  \n",
      " The Transformer is a model architecture that uses self-attention mechanisms to draw global dependencies between input and output, allowing for more parallelization and reaching a new state of the art in translation quality. Previous work such as Extended Neural GPU, ByteNet, and ConvS2S have used convolutional neural networks to compute hidden representations in parallel, but the Transformer takes this one step further by relying entirely on self-attention. \n",
      "The Transformer model architecture consists of an encoder and a decoder stack. The encoder is composed of a stack of N=6 identical layers, each of which has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder also has three sub-layers: a multi-head attention layer over the output of the encoder stack, a self-attention layer to prevent positions from attending to subsequent positions, and a fully connected feed-forward network. Attention is a function that maps a  Scaled dot-product attention is a type of attention mechanism used in natural language processing and machine learning. It consists of queries, keys, and values which are combined through matrix multiplication and scaled by the square root of the number of dimensions. Multi-head attention uses multiple attention layers running in parallel to increase efficiency. \n",
      " Multi-head attention allows the model to attend to different representation subspaces at different positions. It uses multiple attention heads, each of which has its own parameter matrices for the queries, keys and values. These are then concatenated together and applied to the output with a weight matrix. This is used in the encoder-decoder attention layers, self-attention layers in the encoder, and self-attention layers in the decoder with masking to preserve the auto-regressive property. In addition, position-wise feed-forward networks, embeddings, \n",
      "This paper compares the use of self-attention layers to recurrent and convolutional layers for sequence transduction tasks. Self-attention layers have a constant computational complexity per layer, whereas recurrent and convolutional layers have O(n) and O(k*n) complexities respectively. In terms of path length between long-range dependencies, self-attention layers have shorter paths than recurrent or convolutional layers. We also discuss the use of positional encodings to improve performance for longer sequences.  We describe our training regime for the Transformer model, including data and batching, hardware and schedule, optimizer, and regularization. We use Adam optimizer with warmup steps and residual dropout.  The Transformer model achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. It surpasses all previously published models and ensembles with its base model, achieving 28.4 BLEU for the English-to-German translation task and 41.0 BLEU for the English-to-French translation task. Label smoothing during training is also used to improve accuracy and BLEU score.  In Table 3, reducing the attention key size dk hurts model quality, suggesting that determining compatibility is not easy. Bigger models are better, and dropout is very helpful in avoiding over-fitting. Replacing our sinusoidal positional encoding with learned positional embeddings results in nearly identical results to the base model. In Table 4, experiments on English constituency parsing show that the Transformer can generalize to other tasks, achieving competitive results. \n",
      " \n",
      "This paper presents the Transformer, a sequence transduction model based entirely on attention. It is shown to be able to outperform other architectures based on recurrent or convolutional layers in translation tasks. The Transformer is trained faster and achieves state-of-the-art results for English-to-German and English-to-French translation tasks. We plan to extend the Transformer to other tasks and investigate local, restricted attention mechanisms for large inputs and outputs. \n",
      "This paper reviews the major developments in recurrent neural network (RNN) models for sequence modeling. It covers topics such as long short-term memory (LSTM), gated recurrent units (GRUs), and self-attention networks. It also discusses recent applications of RNNs, such as machine translation, natural language understanding, and image captioning. Finally, it provides an overview of some of the most popular datasets used to evaluate RNN models. \n",
      " \n",
      "This paper presents a review of recent advances in neural machine translation, summarization, parsing and language modeling. It covers models such as encoder-decoders, attention mechanisms, tree-based models, deep reinforced models, output embeddings, subword units, sparsely-gated mixture-of-experts layers, and end-to-end memory networks. It also discusses popular approaches such as dropout, Google's Neural Machine Translation system, Fast and Accurate Shift-Reduce Constituent Parsing, and Grammar as a Foreign Language.  The attention mechanism in layer 5 of the encoder self-attention follows long-distance dependencies, and many of the attention heads attend to a distant dependency of the verb \"making,\" completing the phrase \"making...more difficult.\" \n",
      "Attention heads in layer 5 of a 6-layer network appear to be involved in anaphora resolution, as evidenced by the sharp attentions seen when focusing on the word \"its\".  Attention heads in the encoder self-attention layers of a Transformer model can be observed to perform different tasks related to the structure of the sentence, demonstrating their ability to learn useful features from the data.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question answer with text embedding in summarized paper."
   ],
   "metadata": {
    "id": "xQMLwSRvnFmY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity"
   ],
   "metadata": {
    "id": "HTs5Vq6wm6dk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(summary_list)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ],
   "metadata": {
    "id": "mVUpQJkEnEHN",
    "outputId": "0dee4feb-d55a-4dd2-b65e-ef1374018b4f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15, 1)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   0\n",
       "0  \\n This paper proposes a new sequence transduc...\n",
       "1   \\n The Transformer is a model architecture th...\n",
       "2  \\nThe Transformer model architecture consists ...\n",
       "3   Scaled dot-product attention is a type of att...\n",
       "4  \\n Multi-head attention allows the model to at..."
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-66887631-d31a-44dd-9917-7cd9c8ea7e22\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n This paper proposes a new sequence transduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n The Transformer is a model architecture th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nThe Transformer model architecture consists ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scaled dot-product attention is a type of att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n Multi-head attention allows the model to at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66887631-d31a-44dd-9917-7cd9c8ea7e22')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-66887631-d31a-44dd-9917-7cd9c8ea7e22 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-66887631-d31a-44dd-9917-7cd9c8ea7e22');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embeddings = df[0].astype(str).apply([lambda x: get_embedding(x, engine=embedding_model)])\n",
    "df[\"embeddings\"] = embeddings"
   ],
   "metadata": {
    "id": "NtoKWzxUnZu7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.shape"
   ],
   "metadata": {
    "id": "ereaEk9BnbOH",
    "outputId": "75b8443b-6aae-423b-8453-6e9bcbc0af99",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def search_reviews(df, query):\n",
    "    query_embedding = get_embedding(\n",
    "        query,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    df[\"similarity\"] = df.embeddings.apply(lambda x: cosine_similarity(x, query_embedding))\n",
    "\n",
    "    results = (\n",
    "        df.sort_values(\"similarity\", ascending=False)\n",
    "    )\n",
    "    \n",
    "    return results"
   ],
   "metadata": {
    "id": "DTZCxKqYn-aA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results = search_reviews(df, \"what is this paper about?\")\n",
    "results.iloc[0][0]\n"
   ],
   "metadata": {
    "id": "UFMajySxoCsD",
    "outputId": "5a0b30b2-b606-43eb-bd40-9af1070cafc8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n \\nThis paper presents a review of recent advances in neural machine translation, summarization, parsing and language modeling. It covers models such as encoder-decoders, attention mechanisms, tree-based models, deep reinforced models, output embeddings, subword units, sparsely-gated mixture-of-experts layers, and end-to-end memory networks. It also discusses popular approaches such as dropout, Google's Neural Machine Translation system, Fast and Accurate Shift-Reduce Constituent Parsing, and Grammar as a Foreign Language.\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 29
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "results = search_reviews(df, \"explain the training procedure\")\n",
    "results.iloc[0][0]"
   ],
   "metadata": {
    "id": "UsXZghv3oFOq",
    "outputId": "8d7f6319-106a-4867-f4de-6fb817a01d89",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' We describe our training regime for the Transformer model, including data and batching, hardware and schedule, optimizer, and regularization. We use Adam optimizer with warmup steps and residual dropout.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "results = search_reviews(df, \"Which one is better, transformer or RNN\")\n",
    "results.iloc[0][0]"
   ],
   "metadata": {
    "id": "nLSxDaKLokI3",
    "outputId": "74274169-8715-4ff4-a8ea-afd1cd1fd6ad",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n \\nThis paper presents the Transformer, a sequence transduction model based entirely on attention. It is shown to be able to outperform other architectures based on recurrent or convolutional layers in translation tasks. The Transformer is trained faster and achieves state-of-the-art results for English-to-German and English-to-French translation tasks. We plan to extend the Transformer to other tasks and investigate local, restricted attention mechanisms for large inputs and outputs.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 31
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rerun with summarized text give even more summarized version."
   ],
   "metadata": {
    "id": "FaxObNudm8Hn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prompt= summary_text + \"\\n Tl;dr:\"\n",
    "response = openai.Completion.create(\n",
    "model=\"text-davinci-003\",\n",
    "prompt=prompt,\n",
    "temperature=0.7,\n",
    "max_tokens=400,\n",
    "top_p=0.9,\n",
    "frequency_penalty=0.0,\n",
    "presence_penalty=1\n",
    ")\n"
   ],
   "metadata": {
    "id": "lrN3AgYzZHzd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "short_summary = response[\"choices\"][0][\"text\"]\n",
    "print(\"The short summary is: \\n\", short_summary)"
   ],
   "metadata": {
    "id": "YIws7vilpAK9",
    "outputId": "447302d5-d04f-4489-9a0c-3182851f1a9a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The short summary is: \n",
      "  This paper reviews recent advances in sequence transduction models based on attention mechanisms, such as the Transformer model. The Transformer is shown to be superior in quality to existing models while requiring significantly less time to train. Experiments on machine translation tasks and English constituency parsing show that the Transformer model achieves state-of-the-art results. Self-attention layers are used in the encoder and decoder stack and allow for more parallelization than recurrent or convolutional layers.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Np0zcc0IajDP",
    "outputId": "080ce32a-c04b-40f9-fd65-2a2b806520b4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" This paper presents a new architecture, the Transformer, which relies on attention mechanisms instead of recurrence and is more parallelizable. Experiments on two machine translation tasks show that the model achieves superior results, improving over the existing best results by 2 BLEU points. It contains an encoder and decoder stack with multi-head self-attention mechanisms and position-wise fully connected feed-forward networks. Attention heads in the encoder self-attention layer have learned to perform different tasks related to the structure of the sentence.\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1670219783,\n",
      "  \"id\": \"cmpl-6Jykhkghj0VSQG0tokyjPRpg6w9OA\",\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 108,\n",
      "    \"prompt_tokens\": 1262,\n",
      "    \"total_tokens\": 1370\n",
      "  }\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "summary_list =[]\n",
    "for page in doc:\n",
    "  text = page.get_text(\"text\")\n",
    "  #print(text)\n",
    "  prompt= \"Summarize this for a second-grade student: \" +text \n",
    "  response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=prompt,\n",
    "  temperature=0.7,\n",
    "  max_tokens=120,\n",
    "  top_p=0.9,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=1\n",
    "  )\n",
    "  summary_list.append(response[\"choices\"][0][\"text\"])\n"
   ],
   "metadata": {
    "id": "r4WmT6W5ePIk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "summary_text= ' '.join(summary_list)\n",
    "print(summary_text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUYSaUW6f8-H",
    "outputId": "e5f381a3-b882-4101-e96a-e58f2ce35419"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Scientists have created a new type of network architecture called the Transformer that uses attention mechanisms instead of recurrence and convolutions to help machines understand and translate language. Experiments have shown that this model is faster and more accurate than other models that have been used before. It has helped machines to do things like translate English into German or French and understand how words fit together in a sentence. A Transformer is a type of model used for language modeling and machine translation. It uses attention mechanisms to draw global dependencies between input and output, which allows for more parallelization and faster training times. For a second-grade student:\n",
      "The Transformer is a model that helps machines understand and generate language. It has two parts - an encoder which takes in the language and a decoder which produces the output language. It also uses something called attention, which is like a way for the machine to remember important words from the input language and use them in the output language. \n",
      "Multi-Head Attention is a type of attention used in computer programming. It consists of several layers that run in parallel, each layer taking the queries, keys, and values and projecting them into different dimensions. The output is then combined and projected to create the final values. Scaled Dot-Product Attention is a type of Multi-Head Attention which scales the dot products by 1 divided by the square root of the dimension of the keys. This helps make sure that the softmax function works correctly. tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [18].\n",
      "\n",
      "Multi-head attention is a way for a computer model to look at different parts of a sentence or other text at the same time. It uses 8 different \" Self-attention is a way of connecting each part of a sequence to every other part. It can be used to help machines understand longer sequences, like sentences. Self-attention layers take less time than recurrent layers and have shorter paths between long-range dependencies. A second-grade student might understand this as: Scientists are trying to create a better way for computers to understand language. They are using something called \"self-attention\" which helps computers understand the words and their meanings in sentences. They use layers of different parts in their models to help with this. This process is complex and takes time, but it may help computers better understand language. A second-grade student can understand that the Transformer is a machine learning model that can achieve better scores than other models on English-to-German and English-to-French tests, while using less training cost. It achieved the highest BLEU score of 28.4 on English-to-German translation and 41.0 on English-to-French translation. \n",
      "The Transformer is a type of architecture that can help with translating and understanding language. It can be changed in different ways to make it work better. When the size of the model is bigger and dropout is used, it works better. The Transformer also works well when doing English constituency parsing which is a type of language understanding. A computer model called the Transformer was trained on 40,000 sentences to learn how to translate between languages. It was able to outperform other models and is now used to help computers understand human language better. Four people (Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio) studied how computers can understand things that are written or said. They used something called a recurrent neural network to help them. Four people, Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit, wrote a paper about a special type of computer model. Romain Paulus, Caiming Xiong, and Richard Socher wrote a paper about a deep learning model that helps computers summarize information. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein wrote a paper about teaching computers to understand tree diagrams. Oﬁr Press and Lior Wolf wrote a paper about making computers better at language. Rico Senn \n",
      "Many governments in America have passed new laws since 2009 which make it harder to register or vote. We need to make sure that the law is applied fairly and justly, which is something we are missing. The law can never be perfect, but it should be applied in a just way - this is something we are missing.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prompt= \"Summarize this for a second-grade student: \"+summary_text \n",
    "response = openai.Completion.create(\n",
    "model=\"text-davinci-003\",\n",
    "prompt=prompt,\n",
    "temperature=0.7,\n",
    "max_tokens=400,\n",
    "top_p=0.9,\n",
    "frequency_penalty=0.0,\n",
    "presence_penalty=1\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nScXhI01gDiK",
    "outputId": "d64e1c82-ecea-4bb8-c42e-72e7f9f45afe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Scientists have been looking for ways to make computers understand language better. They have created a model called the Transformer that uses attention mechanisms and is able to translate quickly and accurately. This model has been tested on two machine translation tasks and was found to be more accurate and faster than existing methods. It also works well for English constituency parsing.\n"
     ]
    }
   ]
  }
 ]
}