{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 0. Installing Transformers and Importing Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:31:36.066807Z","iopub.status.busy":"2023-02-03T02:31:36.065443Z","iopub.status.idle":"2023-02-03T02:31:49.066652Z","shell.execute_reply":"2023-02-03T02:31:49.065410Z","shell.execute_reply.started":"2023-02-03T02:31:36.066678Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install --quiet transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:31:49.070241Z","iopub.status.busy":"2023-02-03T02:31:49.069809Z","iopub.status.idle":"2023-02-03T02:32:06.069233Z","shell.execute_reply":"2023-02-03T02:32:06.068044Z","shell.execute_reply.started":"2023-02-03T02:31:49.070194Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install --quiet --upgrade arxiv pypdf pymupdf"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:37:50.063939Z","iopub.status.busy":"2023-02-03T02:37:50.063071Z","iopub.status.idle":"2023-02-03T02:38:26.705449Z","shell.execute_reply":"2023-02-03T02:38:26.704255Z","shell.execute_reply.started":"2023-02-03T02:37:50.063901Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install --quiet openai"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:32:06.072667Z","iopub.status.busy":"2023-02-03T02:32:06.070600Z","iopub.status.idle":"2023-02-03T02:32:15.265591Z","shell.execute_reply":"2023-02-03T02:32:15.264637Z","shell.execute_reply.started":"2023-02-03T02:32:06.072611Z"},"trusted":true},"outputs":[],"source":["from transformers import pipeline\n","from bs4 import BeautifulSoup\n","import requests\n","import arxiv\n","from pypdf import PdfReader\n","import fitz\n"]},{"cell_type":"markdown","metadata":{},"source":["# download pdf file"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:32:18.888018Z","iopub.status.busy":"2023-02-03T02:32:18.887364Z","iopub.status.idle":"2023-02-03T02:32:18.894226Z","shell.execute_reply":"2023-02-03T02:32:18.893254Z","shell.execute_reply.started":"2023-02-03T02:32:18.887983Z"},"trusted":true},"outputs":[],"source":["def download_arxiv(url):\n","    id_from_url = url.split(\"/\")[-1:]\n","    paper = next(arxiv.Search(id_list=id_from_url).results())\n","    print(paper.title)\n","    print(paper.authors)\n","    print(paper.primary_category)\n","    print(paper.published)\n","    print(paper.summary)\n","    # Download the PDF to a specified directory with a custom filename.\n","    paper.download_pdf(dirpath='./', filename=\"downloaded-paper.pdf\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:32:19.294491Z","iopub.status.busy":"2023-02-03T02:32:19.293688Z","iopub.status.idle":"2023-02-03T02:32:25.411208Z","shell.execute_reply":"2023-02-03T02:32:25.409954Z","shell.execute_reply.started":"2023-02-03T02:32:19.294454Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attention Is All You Need\n","[arxiv.Result.Author('Ashish Vaswani'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('Niki Parmar'), arxiv.Result.Author('Jakob Uszkoreit'), arxiv.Result.Author('Llion Jones'), arxiv.Result.Author('Aidan N. Gomez'), arxiv.Result.Author('Lukasz Kaiser'), arxiv.Result.Author('Illia Polosukhin')]\n","cs.CL\n","2017-06-12 17:57:34+00:00\n","The dominant sequence transduction models are based on complex recurrent or\n","convolutional neural networks in an encoder-decoder configuration. The best\n","performing models also connect the encoder and decoder through an attention\n","mechanism. We propose a new simple network architecture, the Transformer, based\n","solely on attention mechanisms, dispensing with recurrence and convolutions\n","entirely. Experiments on two machine translation tasks show these models to be\n","superior in quality while being more parallelizable and requiring significantly\n","less time to train. Our model achieves 28.4 BLEU on the WMT 2014\n","English-to-German translation task, improving over the existing best results,\n","including ensembles by over 2 BLEU. On the WMT 2014 English-to-French\n","translation task, our model establishes a new single-model state-of-the-art\n","BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\n","of the training costs of the best models from the literature. We show that the\n","Transformer generalizes well to other tasks by applying it successfully to\n","English constituency parsing both with large and limited training data.\n"]}],"source":["url = \"https://arxiv.org/abs/1706.03762\"\n","download_arxiv(url)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:32:25.413774Z","iopub.status.busy":"2023-02-03T02:32:25.413425Z","iopub.status.idle":"2023-02-03T02:32:26.407446Z","shell.execute_reply":"2023-02-03T02:32:26.406139Z","shell.execute_reply.started":"2023-02-03T02:32:25.413734Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__notebook_source__.ipynb  downloaded-paper.pdf\n"]}],"source":["!ls /kaggle/working"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:47:49.709559Z","iopub.status.busy":"2023-02-03T02:47:49.709189Z","iopub.status.idle":"2023-02-03T02:48:03.839309Z","shell.execute_reply":"2023-02-03T02:48:03.838080Z","shell.execute_reply.started":"2023-02-03T02:47:49.709529Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install --quiet spacy contractions word2number"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess file with NLTK"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:48:03.842428Z","iopub.status.busy":"2023-02-03T02:48:03.842024Z","iopub.status.idle":"2023-02-03T02:48:23.798658Z","shell.execute_reply":"2023-02-03T02:48:23.797538Z","shell.execute_reply.started":"2023-02-03T02:48:03.842386Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading spaCy model en_core_web_md\n","Collecting en-core-web-md==3.3.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0-py3-none-any.whl (33.5 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.5/33.5 MB 8.9 MB/s eta 0:00:00\n","Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-md==3.3.0) (3.3.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.9)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.4.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.8)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.10.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.8.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.2)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.1.2)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (23.0)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.1.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.64.0)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.7.9)\n","Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.21.6)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.12)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (6.3.0)\n","Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.10.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.8)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.28.1)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.17)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.7)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (59.8.0)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.8.0)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.26.14)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.2)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (6.0.0)\n","Installing collected packages: en-core-web-md\n","Successfully installed en-core-web-md-3.3.0\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_md')\n","Finished downloading model\n"]}],"source":["from bs4 import BeautifulSoup\n","import spacy\n","import unidecode\n","from word2number import w2n\n","import contractions\n","\n","def download_spacy_model(model=\"en_core_web_md\"):\n","    print(f\"Downloading spaCy model {model}\")\n","    spacy.cli.download(model)\n","    print(f\"Finished downloading model\")\n","\n","download_spacy_model()\n","\n","nlp = spacy.load('en_core_web_md')\n","\n","# exclude words from spacy stopwords list\n","deselect_stop_words = ['no', 'not']\n","for w in deselect_stop_words:\n","    nlp.vocab[w].is_stop = False\n","\n","\n","    \n","def strip_html_tags(text):\n","    \"\"\"remove html tags from text\"\"\"\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    stripped_text = soup.get_text(separator=\" \")\n","    return stripped_text\n","\n","\n","def remove_whitespace(text):\n","    \"\"\"remove extra whitespaces from text\"\"\"\n","    text = text.strip()\n","    return \" \".join(text.split())\n","\n","\n","def remove_accented_chars(text):\n","    \"\"\"remove accented characters from text, e.g. café\"\"\"\n","    text = unidecode.unidecode(text)\n","    return text\n","\n","\n","def expand_contractions(text):\n","    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n","    text = contractions.fix(text)\n","    return text\n","\n","\n","def text_preprocessing(text, accented_chars=True, contractions=True, \n","                       convert_num=True, extra_whitespace=True, \n","                       lemmatization=True, lowercase=True, punctuations=True,\n","                       remove_html=True, remove_num=True, special_chars=True, \n","                       stop_words=True):\n","    \"\"\"preprocess text with default option set to true for all steps\"\"\"\n","    if remove_html == True: #remove html tags\n","        text = strip_html_tags(text)\n","    if extra_whitespace == True: #remove extra whitespaces\n","        text = remove_whitespace(text)\n","    if accented_chars == True: #remove accented characters\n","        text = remove_accented_chars(text)\n","    if contractions == True: #expand contractions\n","        text = expand_contractions(text)\n","    if lowercase == True: #convert all characters to lowercase\n","        text = text.lower()\n","\n","    doc = nlp(text) #tokenise text\n","\n","    clean_text = []\n","    \n","    for token in doc:\n","        flag = True\n","        edit = token.text\n","        # remove stop words\n","        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n","            flag = False\n","        # remove punctuations\n","        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n","            flag = False\n","        # remove special characters\n","        if special_chars == True and token.pos_ == 'SYM' and flag == True: \n","            flag = False\n","        # remove numbers\n","        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n","        and flag == True:\n","            flag = False\n","        # convert number words to numeric numbers\n","        if convert_num == True and token.pos_ == 'NUM' and flag == True:\n","            edit = w2n.word_to_num(token.text)\n","        # convert tokens to base form\n","        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n","            edit = token.lemma_\n","        # append tokens edited and not removed to list \n","        if edit != \"\" and flag == True:\n","            clean_text.append(edit)        \n","    return clean_text"]},{"cell_type":"markdown","metadata":{},"source":["## Read the pdf file content"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:32:26.430430Z","iopub.status.busy":"2023-02-03T02:32:26.429698Z","iopub.status.idle":"2023-02-03T02:32:26.445272Z","shell.execute_reply":"2023-02-03T02:32:26.444287Z","shell.execute_reply.started":"2023-02-03T02:32:26.430391Z"},"trusted":true},"outputs":[],"source":["# Read the pdf file and read its content:\n","def parse_paper(path):\n","    print(\"Parsing paper\")\n","    reader = PdfReader(path)\n","    number_of_pages = len(reader.pages)\n","    print(f\"Total number of pages: {number_of_pages}\")\n","    paper_text = []\n","    for i in range(number_of_pages):\n","        page = reader.pages[i]\n","        page_text = []\n","\n","        def visitor_body(text, cm, tm, fontDict, fontSize):\n","            x = tm[4]\n","            y = tm[5]\n","            # ignore header/footer\n","            if (y > 50 and y < 720) and (len(text.strip()) > 1) and (fontSize<=10 and fontSize>=6) and len(text)>10:\n","                page_text.append({\n","                    'fontsize': fontSize,\n","                    'text': text.strip().replace('\\x03', ''),\n","                    'x': x,\n","                    'y': y\n","                })\n","\n","        _ = page.extract_text(visitor_text=visitor_body)\n","\n","        blob_font_size = None\n","        blob_text = ''\n","        processed_text = []\n","\n","        for t in page_text:\n","            if t['fontsize'] == blob_font_size:\n","                blob_text += f\" {t['text']}\"\n","            else:\n","                if blob_font_size is not None and len(blob_text) > 1:\n","                    processed_text.append({\n","                        'fontsize': blob_font_size,\n","                        'text': blob_text,\n","                        'page': i\n","                    })\n","                blob_font_size = t['fontsize']\n","                blob_text = t['text']\n","        paper_text += processed_text\n","    return paper_text\n","\n","def parse_paper2(path):\n","    print(\"Parsing paper...\")\n","    doc = fitz.open(path)\n","    print(\"TotalPages of doc: \", len(doc))\n","    text_list = []\n","    for page in doc:\n","        text = page.get_text(\"text\", sort=True)\n","        text_list.append(text)\n","    return \" \".join(text_list)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:32:26.447328Z","iopub.status.busy":"2023-02-03T02:32:26.446921Z","iopub.status.idle":"2023-02-03T02:32:26.458985Z","shell.execute_reply":"2023-02-03T02:32:26.457983Z","shell.execute_reply.started":"2023-02-03T02:32:26.447234Z"},"trusted":true},"outputs":[],"source":["import re\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:32:26.461822Z","iopub.status.busy":"2023-02-03T02:32:26.461003Z","iopub.status.idle":"2023-02-03T02:32:26.469149Z","shell.execute_reply":"2023-02-03T02:32:26.467912Z","shell.execute_reply.started":"2023-02-03T02:32:26.461784Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    index = text.find(\"Abstract\")\n","    print(\"Remove before abstract!!\")\n","    text = text[index+8:]\n","    # return list of preprocessed text\n","#     text = text_preprocessing(text, lowercase=False, punctuations=False)\n","#     text = ' '.join(text)\n","    text = re.sub(r'\\[.*?\\]', '', text) # remove square brackets\n","    text = text.replace(\"\\n\", '') # remove \\n\n","    text = re.sub(r'[^\\x00-\\x7F]+',' ', text) # remove non-ASCII characters\n","    return text"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:32:26.471449Z","iopub.status.busy":"2023-02-03T02:32:26.470723Z","iopub.status.idle":"2023-02-03T02:32:26.623333Z","shell.execute_reply":"2023-02-03T02:32:26.622190Z","shell.execute_reply.started":"2023-02-03T02:32:26.471415Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Parsing paper...\n","TotalPages of doc:  15\n","Remove before abstract!!\n"]}],"source":["path_to_pdf = \"/kaggle/working/downloaded-paper.pdf\"\n","text = parse_paper2(path_to_pdf)\n","text = preprocess_text(text)"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Load Summarization Pipeline"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:32:29.297262Z","iopub.status.busy":"2023-02-03T02:32:29.296871Z","iopub.status.idle":"2023-02-03T02:34:52.043037Z","shell.execute_reply":"2023-02-03T02:34:52.041983Z","shell.execute_reply.started":"2023-02-03T02:32:29.297221Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9a35002d7994e2dba3f9711631f644e","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.76k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-02-03 02:32:32.058605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-03 02:32:32.059846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-03 02:32:32.060515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-03 02:32:32.062723: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-02-03 02:32:32.063084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-03 02:32:32.063818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-03 02:32:32.064557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-03 02:32:36.922977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-03 02:32:36.923872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-03 02:32:36.924548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-03 02:32:36.925134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15043 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93b1149767ce4b4da256de2336b9b740","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0be9a90af84f40c2b6a9eb5fcf65ca6f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a87645b2f2c4f818bd665072ecc71a4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4857f346b6db4617a1d89a5b2113d53d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["summarizer = pipeline(\"summarization\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Chunk Text"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:34:52.046147Z","iopub.status.busy":"2023-02-03T02:34:52.045740Z","iopub.status.idle":"2023-02-03T02:34:52.052884Z","shell.execute_reply":"2023-02-03T02:34:52.051819Z","shell.execute_reply.started":"2023-02-03T02:34:52.046108Z"},"trusted":true},"outputs":[],"source":["max_chunk = 500\n","text = text.replace('.', '.<eos>')\n","text = text.replace('?', '?<eos>')\n","text = text.replace('!', '!<eos>')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:34:52.054755Z","iopub.status.busy":"2023-02-03T02:34:52.054390Z","iopub.status.idle":"2023-02-03T02:34:52.427553Z","shell.execute_reply":"2023-02-03T02:34:52.426166Z","shell.execute_reply.started":"2023-02-03T02:34:52.054712Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]}],"source":["sentences = text.split('<eos>')\n","current_chunk = 0 \n","chunks = []\n","for sentence in sentences:\n","    if len(chunks) == current_chunk + 1: \n","        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n","            chunks[current_chunk].extend(sentence.split(' '))\n","        else:\n","            current_chunk += 1\n","            chunks.append(sentence.split(' '))\n","    else:\n","        print(current_chunk)\n","        chunks.append(sentence.split(' '))\n","\n","for chunk_id in range(len(chunks)):\n","    chunks[chunk_id] = ' '.join(chunks[chunk_id])"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:34:52.430693Z","iopub.status.busy":"2023-02-03T02:34:52.430065Z","iopub.status.idle":"2023-02-03T02:34:52.441800Z","shell.execute_reply":"2023-02-03T02:34:52.440359Z","shell.execute_reply.started":"2023-02-03T02:34:52.430652Z"},"trusted":true},"outputs":[{"data":{"text/plain":["12"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["len(chunks)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Summarize Text"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:34:57.039830Z","iopub.status.busy":"2023-02-03T02:34:57.039437Z","iopub.status.idle":"2023-02-03T02:37:49.969503Z","shell.execute_reply":"2023-02-03T02:37:49.968486Z","shell.execute_reply.started":"2023-02-03T02:34:57.039792Z"},"trusted":true},"outputs":[],"source":["res = summarizer(chunks, max_length=120, min_length=30, do_sample=False)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:37:49.972039Z","iopub.status.busy":"2023-02-03T02:37:49.971639Z","iopub.status.idle":"2023-02-03T02:37:49.980656Z","shell.execute_reply":"2023-02-03T02:37:49.979789Z","shell.execute_reply.started":"2023-02-03T02:37:49.972002Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'summary_text': ' We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions . Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signi cantlyless time to train .'}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["res[0]\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:38:35.870733Z","iopub.status.busy":"2023-02-03T02:38:35.870340Z","iopub.status.idle":"2023-02-03T02:38:35.876744Z","shell.execute_reply":"2023-02-03T02:38:35.875686Z","shell.execute_reply.started":"2023-02-03T02:38:35.870696Z"},"trusted":true},"outputs":[],"source":["result_summary_hf = ' '.join([summ['summary_text'] for summ in res])\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:38:47.599911Z","iopub.status.busy":"2023-02-03T02:38:47.598829Z","iopub.status.idle":"2023-02-03T02:38:47.605367Z","shell.execute_reply":"2023-02-03T02:38:47.604409Z","shell.execute_reply.started":"2023-02-03T02:38:47.599847Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions . Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signi cantlyless time to train .  Attention mechanisms have become an integral part of compelling sequence modeling and transduc-tion models in various tasks . In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism . The Transformer allows for signi cantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs .  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2) Multi-Head Attention consists of severalattention layers running in parallel . We employ residual connections around each of the sub-layers, followed by layer normalization . We also modify the self-attentionsub-layer in the decoder stack .  Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions . In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer . In a self-attention layer all of the keys, valuesand queries come in the same place, in this case, the output of the previous layer in the encoder . In addition to attention sub-layers, each of the layers in our model contains a fullyconnected feed-forward network .  We use learned embeddings to convert input and output tokens to vectors of dimension dmodel . We also use the usual learned linear transfor-mation and softmax function to convert the decoder output to predicted next-token probabilities . Inour model, we share the same weight matrix between the two embedding layers and the pre-softmaxlinear transformation, similar to .  A self-attention layer connects all positions with a constant number of sequentiallyexecuted operations, whereas a recurrent layer requires O(n) sequential operations . Learning long-rangedependencies is a key challenge in many sequence transduction tasks . The shorter paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies .  The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and French newstest2014 tests . The big transformer model (Transformer (big) outperforms the best previously reported models (including ensembles) by more than 2BLEU .  On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41. 0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model . Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures .  We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank . We also trained it in a semi-supervised setting, using the larger high-con dence and BerkleyParser corpora from with approximately 17M sentences . We observe that reducing the attention key size dk hurts model quality .  We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration . The code we used to train and evaluate our models is available at https://github.com/tensorflow/.  Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.  Factorization tricks for LSTM networks.  ArXiv preprintarXiv:1703. 10722, 2017.  Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, BowenZhou, and Yoshua Bengio .  Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6 . Figure 4: Two attention heads are apparently involved in anaphora resolution . Figure 5: The heads clearly learned to perform different tasks are shown only for the word  making . Different colors represent different heads .\n"]}],"source":["print(result_summary_hf)"]},{"cell_type":"markdown","metadata":{},"source":["### With advance preprocessing"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:43:57.749861Z","iopub.status.busy":"2023-02-03T03:43:57.749295Z","iopub.status.idle":"2023-02-03T03:43:57.771144Z","shell.execute_reply":"2023-02-03T03:43:57.769497Z","shell.execute_reply.started":"2023-02-03T03:43:57.749726Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text, advance_preprocess=False):\n","    # remove abstract\n","    index = text.find('Abstract')\n","    text = text[index+8:]\n","    if advance_preprocess:\n","        #return list of preprocessed text\n","        text = text_preprocessing(text, lowercase=False, punctuations=False)\n","        text = ''.join(text)\n","    text = re.sub(r'\\[.*?\\]', '', text) # remove square brackets\n","    text = text.replace(\"\\n\", '') # remove \\n\n","    text = re.sub(r'[^\\x00-\\x7F]+',' ', text) # remove non-ASCII characters\n","    return text\n","\n","\n","def generate_chunks(text):\n","    max_chunk = 500\n","    text = text.replace('.', '.<eos>')\n","    text = text.replace('?', '?<eos>')\n","    text = text.replace('!', '!<eos>')\n","    sentences = text.split('<eos>')\n","    current_chunk = 0 \n","    chunks = []\n","    for sentence in sentences:\n","        if len(chunks) == current_chunk + 1: \n","            if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n","                chunks[current_chunk].extend(sentence.split(' '))\n","            else:\n","                current_chunk += 1\n","                chunks.append(sentence.split(' '))\n","        else:\n","            print(current_chunk)\n","            chunks.append(sentence.split(' '))\n","\n","    for chunk_id in range(len(chunks)):\n","        chunks[chunk_id] = ' '.join(chunks[chunk_id])\n","    \n","    return chunks\n","        \n","def combine_preprocess_page(doc, advance_preprocess=False):\n","    texts = []\n","    for page in doc:\n","        texts.append(page.get_text(\"text\"))\n","    text = ' '.join(texts)\n","    text = preprocess_text(text, advance_preprocess)\n","    return text\n","        \n","def summarize_text_with_hf(path_to_pdf, advance_preprocess=False):\n","    doc = fitz.open(path_to_pdf)\n","    text = combine_preprocess_page(doc, advance_preprocess=False)\n","#     print(text)\n","    chunks = generate_chunks(text)\n","#     print(chunks)\n","    res = summarizer(chunks, max_length=120, min_length=30, do_sample=False)\n","        \n","    return ' '.join([summ['summary_text'] for summ in res])"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:43:57.985493Z","iopub.status.busy":"2023-02-03T03:43:57.985053Z","iopub.status.idle":"2023-02-03T03:46:46.627843Z","shell.execute_reply":"2023-02-03T03:46:46.626827Z","shell.execute_reply.started":"2023-02-03T03:43:57.985456Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]}],"source":["path_to_pdf = \"/kaggle/working/downloaded-paper.pdf\"\n","result_hf = summarize_text_with_hf(path_to_pdf, advance_preprocess=True)"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:46:46.630395Z","iopub.status.busy":"2023-02-03T03:46:46.629983Z","iopub.status.idle":"2023-02-03T03:46:46.638878Z","shell.execute_reply":"2023-02-03T03:46:46.636707Z","shell.execute_reply.started":"2023-02-03T03:46:46.630357Z"},"trusted":true},"outputs":[{"data":{"text/plain":["' We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions . Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signi cantlyless time to train .  Attention mechanisms have become an integral part of compelling sequence modeling and transduc-tion models in various tasks . In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism . The Transformer allows for signi cantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs .  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2) Multi-Head Attention consists of severalattention layers running in parallel . We employ residual connections around each of the sub-layers, followed by layer normalization . We also modify the self-attentionsub-layer in the decoder stack .  Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions . In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer . In a self-attention layer all of the keys, valuesand queries come in the same place, in this case, the output of the previous layer in the encoder . In addition to attention sub-layers, each of the layers in our model contains a fullyconnected feed-forward network .  We use learned embeddings to convert input and output tokens to vectors of dimension dmodel . We also use the usual learned linear transfor-mation and softmax function to convert the decoder output to predicted next-token probabilities . Inour model, we share the same weight matrix between the two embedding layers and the pre-softmaxlinear transformation, similar to .  A self-attention layer connects all positions with a constant number of sequentiallyexecuted operations, whereas a recurrent layer requires O(n) sequential operations . Learning long-rangedependencies is a key challenge in many sequence transduction tasks . The shorter paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies .  The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and French newstest2014 tests . The big transformer model (Transformer (big) outperforms the best previously reported models (including ensembles) by more than 2BLEU .  On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41. 0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model . We used beam search with a beam size of 4 and length penalty   = 0. 6 . We varied the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant .  We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank . We also trained it in a semi-supervised setting, using the larger high-con dence and BerkleyParser corpora from with approximately 17M sentences . We observe that reducing the attention key size dk hurts model quality .  We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration . The code we used to train and evaluate our models is available at https://github.com/tensorflow/.  Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.  Factorization tricks for LSTM networks.  ArXiv preprintarXiv:1703. 10722, 2017.  Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, BowenZhou, and Yoshua Bengio .  Figure 3: An example of the attention mechanism following long-distance dependencies in theencoder self-attention in layer 5 of 6 . Figure 4: Two different colors represent different heads from the encoder . The image is shown only for the word  making the phrase  making .'"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["result_hf"]},{"cell_type":"markdown","metadata":{},"source":["# OpenAI prediction"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:40:25.731866Z","iopub.status.busy":"2023-02-03T02:40:25.730760Z","iopub.status.idle":"2023-02-03T02:40:25.838987Z","shell.execute_reply":"2023-02-03T02:40:25.837973Z","shell.execute_reply.started":"2023-02-03T02:40:25.731807Z"},"trusted":true},"outputs":[],"source":["import openai\n","openai.api_key =  \"\""]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:49:09.789147Z","iopub.status.busy":"2023-02-03T02:49:09.788501Z","iopub.status.idle":"2023-02-03T02:49:09.803715Z","shell.execute_reply":"2023-02-03T02:49:09.802723Z","shell.execute_reply.started":"2023-02-03T02:49:09.789095Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text, advance_preprocess=False):\n","    if advance_preprocess:\n","        #return list of preprocessed text\n","        text = text_preprocessing(text, lowercase=False, punctuations=False)\n","        text = ''.join(text)\n","    text = re.sub(r'\\[.*?\\]', '', text) # remove square brackets\n","    text = text.replace(\"\\n\", '') # remove \\n\n","    text = re.sub(r'[^\\x00-\\x7F]+',' ', text) # remove non-ASCII characters\n","    return text\n","\n","def summarize_text_with_openai(path_to_pdf, advance_preprocess=False):\n","    doc = fitz.open(path_to_pdf)\n","    summary_list =[]\n","    for page in doc:\n","        text = page.get_text(\"text\")\n","        text = preprocess_text(text, advance_preprocess)\n","        prompt= \"summarize this text: \"+ text + \"\\n Tl;dr:\"\n","        response = openai.Completion.create(\n","            model=\"text-davinci-003\",\n","            prompt=prompt,\n","            temperature=0.5,\n","            max_tokens=120,\n","            top_p=0.9,\n","            frequency_penalty=0.0,\n","            presence_penalty=1\n","          )\n","        summary_list.append(response[\"choices\"][0][\"text\"])\n","        \n","    return ''.join(summary_list)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:50:10.001885Z","iopub.status.busy":"2023-02-03T02:50:10.001376Z","iopub.status.idle":"2023-02-03T02:51:35.914409Z","shell.execute_reply":"2023-02-03T02:51:35.913418Z","shell.execute_reply.started":"2023-02-03T02:50:10.001844Z"},"trusted":true},"outputs":[],"source":["path_to_pdf = \"/kaggle/working/downloaded-paper.pdf\"\n","# summary without advance spacy preprocess\n","summary_text1 = summarize_text_with_openai(path_to_pdf)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:51:35.916801Z","iopub.status.busy":"2023-02-03T02:51:35.916390Z","iopub.status.idle":"2023-02-03T02:51:35.922141Z","shell.execute_reply":"2023-02-03T02:51:35.921187Z","shell.execute_reply.started":"2023-02-03T02:51:35.916744Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" We propose a new model architecture, the Transformer, based solely on attention mechanisms. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing. Our model achieves state-of-the-art results on two machine translation tasks. \n","\n","Transformer is a new network architecture based solely on attention mechanisms. It dispenses with recurrence and convolution entirely, instead relying entirely on self-attention to draw global dependencies between input and output. This makes it much more parallelizable than previous models, leading to faster training times and improved performance. Exper The Transformer is a model architecture that uses self-attention to draw global dependencies between input and output sequences. It allows for significantly more parallelization than recurrent models, and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. The Transformer model is composed of an encoder and a decoder, each of which contains a stack of layers. The encoder has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder also has three sub-layers: a multi-head self-attention mechanism, a position-wise fully connected feed-forward network, and a third sub-layer which performs multi-head attention over the output of the encoder stack. Residual connections and layer normalization are used around each We use multi-head attention to overcome the limitations of dot product attention. Multi-head attention consists of multiple attention layers running in parallel, with different linear projections for the queries, keys and values. This allows us to capture more complex relationships between the input and output. Multi-head attention allows the model to attend to different representation subspaces at different positions, which helps the model learn better. In our model, we use multi-head attention in three ways: encoder-decoder attention, self-attention in the encoder and decoder, and position-wise feed-forward networks. We also use embeddings and a softmax layer, as well as positional encoding to inject information about the relative or absolute position of the tokens in the sequence. Self-attention layers are faster and more powerful than recurrent and convolutional layers for sequence transduction tasks.We proposed a model with self-attention and point-wise feed-forward layers that can be used to process sequences of arbitrary length. We evaluated our model on the WMT 2014 English-German and English-French translation tasks, achieving competitive results compared to recurrent neural network models. Our model is more efficient in terms of computation, as it requires fewer layers and fewer parameters than recurrent models. Additionally, we found that self-attention allows for more interpretable models, as individual attention heads appear to learn tasks related to the syntactic and semantic structure of sentences. The Transformer model achieves state-of-the-art BLEU scores on the English-to-German and English-to-French translation tasks at a fraction of the training cost of previous models. We also evaluate the importance of different components of the Transformer model, finding that the number of attention heads and the attention key and value dimensions have a significant impact on translation quality. We introduce the Transformer, a novel neural network architecture based on self-attention. Itachieves state-of-the-art results on English-to-German translation and is easily adaptable to other tasks. We presented the Transformer, a sequence transduction model based on attention that achieves state-of-the-art results on translation tasks. We showed that it outperforms recurrent and convolutional architectures, and is able to be trained faster. We plan to extend the Transformer to other tasks such as image, audio and video processing.\n","\n","Recurrent Neural Networks (RNNs) have been shown to be effective in sequence modeling tasks such as language modeling, machine translation, and image captioning. Several improvements have been made to RNNs, including the use of gated recurrent units, convolutional sequence to sequence learning, deep residual learning, long short-term memory networks, gradient flow in recurrent nets, generating sequences with recurrent neural networks, self-training probabilistic context-free grammars, structured attention networks, and multi-task sequence to sequence learning. These advances have enabled RNNs to achieve\n","\n","Researchers have developed a variety of neural network models to improve natural language processing tasks such as machine translation, summarization, parsing, and language modeling. These models include recurrent neural networks, attention models, deep reinforced models, tree annotation models, output embedding models, subword unit models, mixture-of-experts layers, dropout regularization, end-to-end memory networks, sequence-to-sequence learning, and inception architectures. These models have been shown to achieve improved accuracy and efficiency in their respective tasks. An example of the attention mechanism in layer 5 of 6 is shown, which attends to a distant dependency of the verb \"making\" to complete the phrase \"making...more difficult\". Different colors represent different heads. Two attention heads in layer 5 of a 6-layer transformer appear to be involved in anaphora resolution, as evidenced by the sharp attentions they give to the word \"its\". Attention heads in a Transformer model can learn different tasks related to the structure of the sentence, as seen in two examples from the encoder self-attention at layer 5 of 6.\n"]}],"source":["print(summary_text1)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:53:39.031812Z","iopub.status.busy":"2023-02-03T02:53:39.031394Z","iopub.status.idle":"2023-02-03T02:55:49.968809Z","shell.execute_reply":"2023-02-03T02:55:49.967821Z","shell.execute_reply.started":"2023-02-03T02:53:39.031754Z"},"trusted":true},"outputs":[],"source":["# summary without advance spacy preprocess\n","summary_text12 = summarize_text_with_openai(path_to_pdf, advance_preprocess=True)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:55:49.971600Z","iopub.status.busy":"2023-02-03T02:55:49.971209Z","iopub.status.idle":"2023-02-03T02:55:49.978185Z","shell.execute_reply":"2023-02-03T02:55:49.977065Z","shell.execute_reply.started":"2023-02-03T02:55:49.971562Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\" We propose a new network architecture, the Transformer, based solely on attention mechanisms, which dispenses with recurrence and convolution entirely. Experiments on machine translation tasks show that models based on the Transformer architecture achieve superior quality to parallelizable RNNs and can be trained significantly faster. Our model achieves a BLEU score of 28.4 on the WMT English-German translation task, improving over existing single-model results, including ensembles, by over 2 BLEU. On the WMT English-French translation task, our model establishes a new single-model state The Transformer is a neural sequence transduction model that relies entirely on self-attention to compute representations of input and output sequences. The model has an encoder-decoder structure, with the encoder mapping an input sequence of symbols to a continuous representation and the decoder generating an output sequence of symbols. Self-attention is used to compute representations of different positions in a single sequence in order to draw global dependencies between input and output. Advantages of this model include parallelization and improved performance on translation tasks. The Transformer model architecture consists of an encoder and a decoder stack, each composed of N identical layers. The encoder has two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. Residual connections are employed between the sub-layers, followed by layer normalization. The output of the embedding layer has dimension dmodel. The decoder is similar to the encoder but adds an additional sub-layer to perform multi-head attention over the output from the encoder stack. As with Scale Dot-Product Attention and Multi-Head Attention are two types of attention layers that are commonly used in deep learning. Scale Dot-Product Attention is a particular type of attention that takes an input consisting of query, key, and value dimensions (d_k, d_v). It computes the dot product between the query and the key, then divides by d_k and applies a softmax function to obtain weight values. Multi-Head Attention instead performs multiple attention functions with different linear projections of the query, key, and value of dimension d_model. The projected versions of the query\\nMulti-head attention allows a model to jointly attend to information in different representation subspaces at different positions. Compared to single-head attention, it can average out the inhibition. The multi-head attention layer takes queries (Q), keys (K) and values (V) as input, and projects them onto separate parameter matrices WQ, WK, WV, and WO, where dmodel is the dimension of the model, and h is the number of heads. This works by having h parallel attention layers, each with its own set of parameters. The use of d Self-attention layers have a maximum path length of O(1) and a minimum number of sequential operations of O(1). Recurrent layers have a maximum path length of O(n) and a minimum number of sequential operations of O(n). Convolutional layers have a maximum path length of O(logk(n)) and a minimum number of sequential operations of O(1). Self-attention with a restricted neighborhood size has a maximum path length of O(nr) and a minimum number of sequential operations of O(are*n*d). To add positional encoding We investigate an approach to increase the maximum path length of a single convolutional layer with kernel width k < n, which does not connect every pair of input and output positions. This requires O(nk) convolutional layers in the case of contiguous kernels, and O(logk(n)) in the case of dilated convolutions. Convolutional layers are generally more expensive than recurrent layers, so we factor k. We also use separable convolutions, which decreases complexity considerably to o(k*n*d+n*d2). If k=n, the Big transformer models outperformed previously published models on both the WMT English-German and English-French translation tasks, achieving state-of-the-art BLEU scores. The model was trained on a P100 GPU over a period of several days and had a fraction of the training cost of competitive models. Variations of the base model were evaluated to measure the importance of different components, with results showing that increasing the number of attention heads and key/value dimensions improved translation quality while keeping computation constant. Beam search was used for inference with a beam size of 5 and length penalty of 1. We observe that increasing the key size of the attention heads (row B) can reduce model quality. We suggest determining compatibility with a more sophisticated compatibility function than dot product, which may be beneficial. For row C and D, we expect that a bigger model would perform better, and that dropout helps to avoid over-fitting. Row E replaces the sinusoidal positional encoding with a learned positional embedding, and we observe nearly identical results as the base model. To evaluate the Transformer's ability to generalize to a new task, we performed experiments on English constituency parsing. This task presents a In this work, we present a Transformer, a sequence transduction model based entirely on self-attention, which replaces the recurrent layers commonly found in encoder-decoder architectures. We train it on a small number of experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank, with a training set of approximately 45K sentences and a vocabulary of K tokens. In semi-supervised settings, the Transformer outperforms the Berkeley Parser on the WSJ training set of 45K sentences. Despite the lack of task-specific tuning, the model performs surprisingly well This paper discusses the empirical evaluation of gate recurrent neural network sequence modeling, recurrent neural network grammar, convolutional sequence-to-sequence learning, generate sequence recurrent neural network, deep residual learning for image recognition, gradient flow recurrent net to learn long-term dependencies, long short-term memory, self-training PCFG grammar for latent annotation language, neural GPU to learn algorithms, active memory to replace attention, explore the limits of language modeling, Adam: A method for stochastic optimization, factorization trick for LSTM networks, structured attention network, neural machine translation in linear time, structured Ankur Parikh, Oscar Tackstrom, Dipanjan Das and Jakob Uszkoreit proposed a decomposable attention model for natural language processing. Romain Paulus, Caiming Xiong and Richard Socher proposed a deep reinforcement model for abstractive summarization. Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein proposed an accurate, compact and interpretable tree annotation. Ofir Press and Lior Wolf proposed an output embedding to improve language models. Rico Sennrich, Barry Haddow and Alexandra Birch proposed a neural machine translation model An example of an attention mechanism following a long-distance dependency in an encoder self-attention layer is shown. Attention is focused on the verb 'making', which completes the phrase 'make...difficult'. Different colors represent different attention heads, which can be seen clearly. Attention heads involve anaphora resolution, where isolated attention words are noted for sharpening the focus of the attention. Attention heads in an encoder self-attention layer can be used to learn different tasks, as seen in the figure.\""]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["summary_text12"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:59:27.287245Z","iopub.status.busy":"2023-02-03T02:59:27.286868Z","iopub.status.idle":"2023-02-03T02:59:27.293099Z","shell.execute_reply":"2023-02-03T02:59:27.292076Z","shell.execute_reply.started":"2023-02-03T02:59:27.287213Z"},"trusted":true},"outputs":[],"source":["# short summary generation\n","def generate_short_summary(summary_text):\n","    prompt= summary_text + \"\\n Tl;dr:\"\n","    response = openai.Completion.create(\n","        model=\"text-davinci-003\",\n","        prompt=prompt,\n","        temperature=0.6,\n","        max_tokens=400,\n","        top_p=0.9,\n","        frequency_penalty=0.0,\n","        presence_penalty=1\n","    )\n","    return response[\"choices\"][0][\"text\"]"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:59:28.129133Z","iopub.status.busy":"2023-02-03T02:59:28.128726Z","iopub.status.idle":"2023-02-03T02:59:39.347415Z","shell.execute_reply":"2023-02-03T02:59:39.346167Z","shell.execute_reply.started":"2023-02-03T02:59:28.129102Z"},"trusted":true},"outputs":[],"source":["final_summary = generate_short_summary(summary_text12)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T02:59:39.352402Z","iopub.status.busy":"2023-02-03T02:59:39.349323Z","iopub.status.idle":"2023-02-03T02:59:39.363201Z","shell.execute_reply":"2023-02-03T02:59:39.362027Z","shell.execute_reply.started":"2023-02-03T02:59:39.352366Z"},"trusted":true},"outputs":[{"data":{"text/plain":["' This paper presents the Transformer, a neural sequence transduction model based solely on attention mechanisms which replaces recurrence and convolution. Experiments show that models based on the Transformer architecture achieve superior quality to parallelizable RNNs and can be trained significantly faster. Our model achieves a BLEU score of 28.4 on the WMT English-German translation task, improving over existing single-model results by over 2 BLEU.'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["final_summary"]},{"cell_type":"markdown","metadata":{},"source":["## Summarize the paper abstract"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:08:26.799814Z","iopub.status.busy":"2023-02-03T03:08:26.799420Z","iopub.status.idle":"2023-02-03T03:08:26.806453Z","shell.execute_reply":"2023-02-03T03:08:26.805165Z","shell.execute_reply.started":"2023-02-03T03:08:26.799759Z"},"trusted":true},"outputs":[],"source":["def download_arxiv(url):\n","    \n","    id_from_url = url.split(\"/\")[-1:]\n","    paper = next(arxiv.Search(id_list=id_from_url).results())\n","#     print(paper.title)\n","    title = paper.title\n","#     print(paper.authors)\n","#     print(paper.primary_category)\n","#     print(paper.published)\n","#     print(paper.summary)\n","    summary = paper.summary\n","#     for a in paper.authors:\n","#         print(str(a))\n","    # Download the PDF to a specified directory with a custom filename.\n","    paper.download_pdf(dirpath='./', filename=\"downloaded-paper.pdf\")\n","    return str(title), str(summary)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:08:27.393291Z","iopub.status.busy":"2023-02-03T03:08:27.392904Z","iopub.status.idle":"2023-02-03T03:08:34.734528Z","shell.execute_reply":"2023-02-03T03:08:34.733286Z","shell.execute_reply.started":"2023-02-03T03:08:27.393258Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'str'> <class 'str'>\n"]}],"source":["url = \"https://arxiv.org/abs/1706.03762\"\n","title, abstract = download_arxiv(url)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:10:43.350190Z","iopub.status.busy":"2023-02-03T03:10:43.349814Z","iopub.status.idle":"2023-02-03T03:10:43.357524Z","shell.execute_reply":"2023-02-03T03:10:43.356365Z","shell.execute_reply.started":"2023-02-03T03:10:43.350159Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"The paper entitle on 'Attention Is All You Need'. The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.\""]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["paper_abstract = f\"The paper entitle on '{title}'. \"+abstract\n","paper_abstract"]},{"cell_type":"markdown","metadata":{},"source":["### With OpenAI"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:11:42.273071Z","iopub.status.busy":"2023-02-03T03:11:42.272672Z","iopub.status.idle":"2023-02-03T03:11:42.278832Z","shell.execute_reply":"2023-02-03T03:11:42.277843Z","shell.execute_reply.started":"2023-02-03T03:11:42.273039Z"},"trusted":true},"outputs":[],"source":["def summarize_abstract(text):\n","    prompt= text + \"\\n Tl;dr:\"\n","    response = openai.Completion.create(\n","        model=\"text-davinci-003\",\n","        prompt=prompt,\n","        temperature=0.7,\n","        max_tokens=400,\n","        top_p=0.9,\n","        frequency_penalty=0.0,\n","        presence_penalty=1\n","    )\n","    return response[\"choices\"][0][\"text\"]"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:12:16.883683Z","iopub.status.busy":"2023-02-03T03:12:16.883314Z","iopub.status.idle":"2023-02-03T03:12:23.079548Z","shell.execute_reply":"2023-02-03T03:12:23.078407Z","shell.execute_reply.started":"2023-02-03T03:12:16.883651Z"},"trusted":true},"outputs":[{"data":{"text/plain":["' This paper proposes a new simple network architecture, the Transformer, based solely on attention mechanisms which has been applied to machine translation and English constituency parsing tasks. It has been found to be superior in quality while being more parallelizable and requiring significantly less time to train than other models.'"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["abstract_openai_summary = summarize_abstract(paper_abstract)\n","abstract_openai_summary"]},{"cell_type":"markdown","metadata":{},"source":["### With Hugging Face"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:18:18.889075Z","iopub.status.busy":"2023-02-03T03:18:18.888666Z","iopub.status.idle":"2023-02-03T03:18:18.894055Z","shell.execute_reply":"2023-02-03T03:18:18.892891Z","shell.execute_reply.started":"2023-02-03T03:18:18.889041Z"},"trusted":true},"outputs":[],"source":["text = paper_abstract\n","text = text.replace('.', '.<eos>')\n","text = text.replace('?', '?<eos>')\n","text = text.replace('!', '!<eos>')"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:18:24.994605Z","iopub.status.busy":"2023-02-03T03:18:24.994221Z","iopub.status.idle":"2023-02-03T03:18:33.367758Z","shell.execute_reply":"2023-02-03T03:18:33.366644Z","shell.execute_reply.started":"2023-02-03T03:18:24.994571Z"},"trusted":true},"outputs":[],"source":["summary_hf_abstract = summarizer(text, max_length=120, min_length=30, do_sample=False)\n"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T03:18:33.370606Z","iopub.status.busy":"2023-02-03T03:18:33.370161Z","iopub.status.idle":"2023-02-03T03:18:33.378969Z","shell.execute_reply":"2023-02-03T03:18:33.377805Z","shell.execute_reply.started":"2023-02-03T03:18:33.370563Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\" The paper entitle on 'Attention Is All You Need' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration . We propose a new simple network architecture, the Transformer, based solely on attention mechanisms .\""]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["summary_hf_abstract[0]['summary_text']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
